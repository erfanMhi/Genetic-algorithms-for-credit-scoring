{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic algorithm-based heuristic for feature selection in credit risk assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import libraries ,we will be needed through this notebook : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import adam\n",
    "from keras.optimizers import SGD\n",
    "from numpy.random import rand as random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = { \n",
    "    'maxFeatureNum': 12,\n",
    "    'minFeatureNum': 5,\n",
    "    'populationSize': 50,\n",
    "    'initProb': .5,\n",
    "    'crossoverProb': .9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "#     print(predictions)\n",
    "#     print(labels)\n",
    "    return (100.0 * np.sum(predictions.reshape(-1) == labels.reshape(-1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Neuralnetwork architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tens_train(X,Y,hiddenNum=40,trainCycles=450,lr=.1,m=.5) :\n",
    "    \"\"\"Train one layer feedforward neural network\n",
    "    Args :\n",
    "       X : training data\n",
    "       Y : training label\n",
    "       hiddenNum : number of hidden units of hidden layer\n",
    "       trainCycles : number of training cycles\n",
    "       lr : learning rate of nueral network\n",
    "       m: momentum of neural network\n",
    "    Returns :\n",
    "       'float' accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    hidden_size = hiddenNum\n",
    "    batch_size = len(X)\n",
    "    attr_size = X.shape[1]\n",
    "    num_steps = 450\n",
    "    num_labels = 1\n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "\n",
    "      # Input data. For the training data, we use a placeholder that will be fed\n",
    "      # at run time with a training minibatch.\n",
    "        tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                        shape=(batch_size, attr_size))\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "#         tf_valid_dataset = tf.constant(valid_dataset)\n",
    "#         tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "        # layer1\n",
    "        weights_1 = tf.Variable(\n",
    "        tf.truncated_normal([attr_size, hidden_size]))\n",
    "        biases_1 = tf.Variable(tf.zeros([hidden_size]))\n",
    "        layer_1 = tf.nn.sigmoid(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "\n",
    "        # layer2  \n",
    "        weights_2 = tf.Variable(tf.truncated_normal([hidden_size,num_labels]))\n",
    "        biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "        # Training computation.\n",
    "        layer_2 = tf.matmul(layer_1,weights_2) + biases_2\n",
    "  \n",
    "        # calculating Loss\n",
    "        loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_train_labels, logits=layer_2))\n",
    "\n",
    "        # Optimizer.\n",
    "        optimizer = tf.train.MomentumOptimizer(lr,m).minimize(loss)\n",
    "        train_prediction = tf.nn.sigmoid(layer_2)\n",
    "        \n",
    "        \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"Initialized\")\n",
    "\n",
    "        \n",
    "        for step in range(trainCycles):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (X.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = X[offset:(offset + batch_size), :]\n",
    "            batch_labels = Y[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            if (step % 1 == 0):\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tens_train(X,Y,hiddenNum=40,trainCycles=450,lr=.1,m=.5) :\n",
    "    \"\"\"Train one layer feedforward neural network\n",
    "    Args :\n",
    "       X : training data\n",
    "       Y : training label\n",
    "       hiddenNum : number of hidden units of hidden layer\n",
    "       trainCycles : number of training cycles\n",
    "       lr : learning rate of nueral network\n",
    "       m: momentum of neural network\n",
    "    Returns :\n",
    "       'float' accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    hidden_size = hiddenNum\n",
    "    batch_size = len(X)\n",
    "    attr_size = X.shape[1]\n",
    "    num_steps = 450\n",
    "    num_labels = Y.shape[1]\n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "\n",
    "      # Input data. For the training data, we use a placeholder that will be fed\n",
    "      # at run time with a training minibatch.\n",
    "        tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                        shape=(batch_size, attr_size))\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "#         tf_valid_dataset = tf.constant(valid_dataset)\n",
    "#         tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "        # layer1\n",
    "        weights_1 = tf.Variable(\n",
    "        tf.truncated_normal([attr_size, hidden_size]))\n",
    "        biases_1 = tf.Variable(tf.zeros([hidden_size]))\n",
    "        layer_1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "\n",
    "        # layer2  \n",
    "        weights_2 = tf.Variable(tf.truncated_normal([hidden_size,num_labels]))\n",
    "        biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "        # Training computation.\n",
    "        layer_2 = tf.matmul(layer_1,weights_2) + biases_2\n",
    "  \n",
    "        # calculating Loss\n",
    "        loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=layer_2))\n",
    "\n",
    "        # Optimizer.\n",
    "        optimizer = tf.train.MomentumOptimizer(lr,m).minimize(loss)\n",
    "        train_prediction = tf.nn.sigmoid(layer_2)\n",
    "        \n",
    "        \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"Initialized\")\n",
    "        feed_dict = {tf_train_dataset : X, tf_train_labels : Y}\n",
    "        _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        print(\"sd sMinibatch accuracy: %.1f%%\" % accuracy(predictions, Y))\n",
    "        for step in range(trainCycles):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "#             offset = (step * batch_size) % (X.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "#             batch_data = X[offset:(offset + batch_size), :]\n",
    "#             batch_labels = Y[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : X, tf_train_labels : Y}\n",
    "            _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            if (step % 1 == 0):\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, Y))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "sd sMinibatch accuracy: 119.6%\n",
      "Minibatch loss at step 0: 167679.531250\n",
      "Minibatch accuracy: 60.0%\n",
      "Minibatch loss at step 1: 0.693016\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2: 0.692297\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 3: 0.692120\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 4: 0.691987\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 5: 0.691860\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 6: 0.691734\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 7: 0.691610\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 8: 0.691488\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 9: 0.691368\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 10: 0.691249\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 11: 0.691132\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 12: 0.691025\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 13: 0.690935\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 14: 0.690846\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 15: 0.690757\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 16: 0.690668\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 17: 0.690579\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 18: 0.690490\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 19: 0.690401\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 20: 0.690313\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 21: 0.690224\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 22: 0.690136\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 23: 0.690048\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 24: 0.689959\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 25: 0.689871\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 26: 0.689784\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 27: 0.689696\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 28: 0.689608\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 29: 0.689520\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 30: 0.689433\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 31: 0.689346\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 32: 0.689259\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 33: 0.689171\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 34: 0.689084\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 35: 0.688998\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 36: 0.688911\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 37: 0.688824\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 38: 0.688738\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 39: 0.688651\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 40: 0.688565\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 41: 0.688480\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 42: 0.688394\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 43: 0.688310\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 44: 0.688224\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 45: 0.688140\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 46: 0.688055\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 47: 0.687970\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 48: 0.687886\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 49: 0.687801\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 50: 0.687717\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 51: 0.687632\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 52: 0.687548\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 53: 0.687464\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 54: 0.687380\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 55: 0.687296\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 56: 0.687212\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 57: 0.687129\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 58: 0.687045\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 59: 0.686962\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 60: 0.686878\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 61: 0.686795\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 62: 0.686712\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 63: 0.686628\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 64: 0.686546\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 65: 0.686462\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 66: 0.686380\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 67: 0.686297\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 68: 0.686214\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 69: 0.686131\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 70: 0.686049\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 71: 0.685967\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 72: 0.685884\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 73: 0.685802\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 74: 0.685720\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 75: 0.685638\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 76: 0.685556\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 77: 0.685474\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 78: 0.685392\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 79: 0.685311\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 80: 0.685229\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 81: 0.685148\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 82: 0.685067\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 83: 0.684985\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 84: 0.684904\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 85: 0.684822\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 86: 0.684742\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 87: 0.684661\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 88: 0.684580\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 89: 0.684499\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 90: 0.684418\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 91: 0.684338\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 92: 0.684257\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 93: 0.684178\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 94: 0.684097\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 95: 0.684017\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 96: 0.683936\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 97: 0.683857\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 98: 0.683777\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 99: 0.683697\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 100: 0.683617\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 101: 0.683537\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 102: 0.683458\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 103: 0.683378\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 104: 0.683299\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 105: 0.683219\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 106: 0.683141\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 107: 0.683061\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 108: 0.682982\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 109: 0.682903\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 110: 0.682824\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 111: 0.682746\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 112: 0.682667\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 113: 0.682589\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 114: 0.682510\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 115: 0.682432\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 116: 0.682353\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 117: 0.682275\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 118: 0.682197\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 119: 0.682119\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 120: 0.682041\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 121: 0.681963\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 122: 0.681885\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 123: 0.681807\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 124: 0.681730\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 125: 0.681652\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 126: 0.681575\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 127: 0.681497\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 128: 0.681420\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 129: 0.681343\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 130: 0.681266\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 131: 0.681189\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 132: 0.681112\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 133: 0.681035\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 134: 0.680958\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 135: 0.680882\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 136: 0.680805\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 137: 0.680729\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 138: 0.680652\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 139: 0.680576\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 140: 0.680499\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 141: 0.680423\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 142: 0.680347\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 143: 0.680271\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 144: 0.680195\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 145: 0.680119\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 146: 0.680044\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 147: 0.679968\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 148: 0.679893\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 149: 0.679817\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 150: 0.679742\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 151: 0.679666\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 152: 0.679591\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 153: 0.679516\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 154: 0.679441\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 155: 0.679366\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 156: 0.679291\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 157: 0.679216\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 158: 0.679141\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 159: 0.679067\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 160: 0.678992\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 161: 0.678918\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 162: 0.678843\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 163: 0.678769\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 164: 0.678695\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 165: 0.678621\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 166: 0.678547\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 167: 0.678473\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 168: 0.678399\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 169: 0.678325\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 170: 0.678251\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 171: 0.678178\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 172: 0.678104\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 173: 0.678030\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 174: 0.677957\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 175: 0.677884\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 176: 0.677811\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 177: 0.677737\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 178: 0.677665\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 179: 0.677591\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 180: 0.677519\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 181: 0.677446\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 182: 0.677373\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 183: 0.677300\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 184: 0.677228\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 185: 0.677155\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 186: 0.677083\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 187: 0.677011\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 188: 0.676938\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 189: 0.676866\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 190: 0.676794\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 191: 0.676712\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 192: 0.676628\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 193: 0.676546\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 194: 0.676463\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 195: 0.676382\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 196: 0.676300\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 197: 0.676219\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 198: 0.676138\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 199: 0.676057\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 200: 0.675976\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 201: 0.675896\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 202: 0.675816\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 203: 0.675737\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 204: 0.675658\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 205: 0.675578\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 206: 0.675500\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 207: 0.675421\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 208: 0.675343\n",
      "Minibatch accuracy: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 209: 0.675265\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 210: 0.675188\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 211: 0.675110\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 212: 0.675033\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 213: 0.674955\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 214: 0.674879\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 215: 0.674802\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 216: 0.674726\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 217: 0.674649\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 218: 0.674574\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 219: 0.674498\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 220: 0.674423\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 221: 0.674347\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 222: 0.674272\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 223: 0.674197\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 224: 0.674122\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 225: 0.674048\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 226: 0.673974\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 227: 0.673899\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 228: 0.673825\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 229: 0.673752\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 230: 0.673678\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 231: 0.673605\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 232: 0.673531\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 233: 0.673459\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 234: 0.673385\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 235: 0.673313\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 236: 0.673240\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 237: 0.673168\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 238: 0.673095\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 239: 0.673024\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 240: 0.672951\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 241: 0.672880\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 242: 0.672808\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 243: 0.672736\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 244: 0.672665\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 245: 0.672594\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 246: 0.672522\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 247: 0.672452\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 248: 0.672381\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 249: 0.672310\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 250: 0.672240\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 251: 0.672169\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 252: 0.672099\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 253: 0.672029\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 254: 0.671959\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 255: 0.671889\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 256: 0.671819\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 257: 0.671750\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 258: 0.671680\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 259: 0.671611\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 260: 0.671542\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 261: 0.671472\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 262: 0.671404\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 263: 0.671335\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 264: 0.671266\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 265: 0.671197\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 266: 0.671129\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 267: 0.671060\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 268: 0.670992\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 269: 0.670924\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 270: 0.670856\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 271: 0.670788\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 272: 0.670720\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 273: 0.670652\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 274: 0.670585\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 275: 0.670517\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 276: 0.670450\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 277: 0.670382\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 278: 0.670316\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 279: 0.670248\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 280: 0.670182\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 281: 0.670114\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 282: 0.670048\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 283: 0.669981\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 284: 0.669915\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 285: 0.669848\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 286: 0.669782\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 287: 0.669715\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 288: 0.669649\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 289: 0.669583\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 290: 0.669518\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 291: 0.669451\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 292: 0.669386\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 293: 0.669320\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 294: 0.669254\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 295: 0.669189\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 296: 0.669124\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 297: 0.669058\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 298: 0.668993\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 299: 0.668928\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 300: 0.668863\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 301: 0.668798\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 302: 0.668732\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 303: 0.668649\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 304: 0.668567\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 305: 0.668485\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 306: 0.668403\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 307: 0.668323\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 308: 0.668243\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 309: 0.668163\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 310: 0.668085\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 311: 0.668007\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 312: 0.667930\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 313: 0.667853\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 314: 0.667777\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 315: 0.667701\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 316: 0.667626\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 317: 0.667552\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 318: 0.667478\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 319: 0.667404\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 320: 0.667331\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 321: 0.667258\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 322: 0.667186\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 323: 0.667114\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 324: 0.667043\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 325: 0.666971\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 326: 0.666901\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 327: 0.666830\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 328: 0.666760\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 329: 0.666690\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 330: 0.666621\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 331: 0.666552\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 332: 0.666483\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 333: 0.666414\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 334: 0.666347\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 335: 0.666278\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 336: 0.666211\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 337: 0.666143\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 338: 0.666077\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 339: 0.666009\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 340: 0.665943\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 341: 0.665876\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 342: 0.665811\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 343: 0.665744\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 344: 0.665679\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 345: 0.665613\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 346: 0.665548\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 347: 0.665482\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 348: 0.665418\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 349: 0.665353\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 350: 0.665289\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 351: 0.665224\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 352: 0.665160\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 353: 0.665096\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 354: 0.665032\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 355: 0.664968\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 356: 0.664905\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 357: 0.664841\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 358: 0.664779\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 359: 0.664715\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 360: 0.664653\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 361: 0.664590\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 362: 0.664527\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 363: 0.664465\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 364: 0.664403\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 365: 0.664340\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 366: 0.664279\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 367: 0.664217\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 368: 0.664155\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 369: 0.664093\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 370: 0.664032\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 371: 0.663971\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 372: 0.663909\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 373: 0.663848\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 374: 0.663787\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 375: 0.663727\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 376: 0.663666\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 377: 0.663605\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 378: 0.663545\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 379: 0.663485\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 380: 0.663424\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 381: 0.663364\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 382: 0.663304\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 383: 0.663244\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 384: 0.663184\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 385: 0.663125\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 386: 0.663065\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 387: 0.663006\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 388: 0.662946\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 389: 0.662887\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 390: 0.662827\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 391: 0.662769\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 392: 0.662709\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 393: 0.662651\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 394: 0.662592\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 395: 0.662533\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 396: 0.662475\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 397: 0.662416\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 398: 0.662358\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 399: 0.662299\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 400: 0.662241\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 401: 0.662183\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 402: 0.662125\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 403: 0.662067\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 404: 0.662009\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 405: 0.661951\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 406: 0.661894\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 407: 0.661836\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 408: 0.661779\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 409: 0.661721\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 410: 0.661664\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 411: 0.661607\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 412: 0.661550\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 413: 0.661492\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 414: 0.661436\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 415: 0.661378\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 416: 0.661322\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 417: 0.661265\n",
      "Minibatch accuracy: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 418: 0.661208\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 419: 0.661152\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 420: 0.661095\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 421: 0.661039\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 422: 0.660982\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 423: 0.660926\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 424: 0.660870\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 425: 0.660814\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 426: 0.660758\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 427: 0.660702\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 428: 0.660646\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 429: 0.660591\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 430: 0.660535\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 431: 0.660479\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 432: 0.660423\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 433: 0.660368\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 434: 0.660312\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 435: 0.660257\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 436: 0.660202\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 437: 0.660147\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 438: 0.660092\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 439: 0.660036\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 440: 0.659982\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 441: 0.659926\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 442: 0.659872\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 443: 0.659817\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 444: 0.659763\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 445: 0.659707\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 446: 0.659653\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 447: 0.659599\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 448: 0.659544\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 449: 0.659490\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 450: 0.659436\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 451: 0.659381\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 452: 0.659327\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 453: 0.659273\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 454: 0.659219\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 455: 0.659165\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 456: 0.659111\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 457: 0.659058\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 458: 0.659004\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 459: 0.658951\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 460: 0.658897\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 461: 0.658844\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 462: 0.658790\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 463: 0.658737\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 464: 0.658683\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 465: 0.658630\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 466: 0.658577\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 467: 0.658523\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 468: 0.658471\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 469: 0.658417\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 470: 0.658365\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 471: 0.658311\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 472: 0.658259\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 473: 0.658206\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 474: 0.658154\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 475: 0.658100\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 476: 0.658037\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 477: 0.657972\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 478: 0.657908\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 479: 0.657845\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 480: 0.657781\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 481: 0.657719\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 482: 0.657656\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 483: 0.657594\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 484: 0.657532\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 485: 0.657471\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 486: 0.657410\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 487: 0.657349\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 488: 0.657288\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 489: 0.657229\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 490: 0.657169\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 491: 0.657109\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 492: 0.657050\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 493: 0.656991\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 494: 0.656932\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 495: 0.656873\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 496: 0.656816\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 497: 0.656757\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 498: 0.656700\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 499: 0.656642\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 500: 0.656585\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 501: 0.656528\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 502: 0.656471\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 503: 0.656415\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 504: 0.656358\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 505: 0.656302\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 506: 0.656246\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 507: 0.656190\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 508: 0.656134\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 509: 0.656079\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 510: 0.656024\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 511: 0.655969\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 512: 0.655914\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 513: 0.655859\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 514: 0.655805\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 515: 0.655750\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 516: 0.655696\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 517: 0.655642\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 518: 0.655588\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 519: 0.655534\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 520: 0.655481\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 521: 0.655427\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 522: 0.655374\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 523: 0.655320\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 524: 0.655267\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 525: 0.655215\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 526: 0.655161\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 527: 0.655109\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 528: 0.655056\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 529: 0.655004\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 530: 0.654952\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 531: 0.654900\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 532: 0.654848\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 533: 0.654796\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 534: 0.654744\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 535: 0.654692\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 536: 0.654641\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 537: 0.654589\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 538: 0.654538\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 539: 0.654486\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 540: 0.654436\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 541: 0.654384\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 542: 0.654333\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 543: 0.654283\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 544: 0.654232\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 545: 0.654182\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 546: 0.654131\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 547: 0.654081\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 548: 0.654030\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 549: 0.653980\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 550: 0.653930\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 551: 0.653880\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 552: 0.653830\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 553: 0.653780\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 554: 0.653730\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 555: 0.653680\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 556: 0.653631\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 557: 0.653581\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 558: 0.653532\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 559: 0.653483\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 560: 0.653433\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 561: 0.653384\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 562: 0.653335\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 563: 0.653287\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 564: 0.653234\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 565: 0.653170\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 566: 0.653105\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 567: 0.653041\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 568: 0.652977\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 569: 0.652914\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 570: 0.652852\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 571: 0.652790\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 572: 0.652730\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 573: 0.652669\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 574: 0.652609\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 575: 0.652549\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 576: 0.652491\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 577: 0.652432\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 578: 0.652374\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 579: 0.652317\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 580: 0.652259\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 581: 0.652202\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 582: 0.652146\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 583: 0.652090\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 584: 0.652034\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 585: 0.651979\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 586: 0.651924\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 587: 0.651869\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 588: 0.651815\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 589: 0.651761\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 590: 0.651707\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 591: 0.651654\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 592: 0.651601\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 593: 0.651548\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 594: 0.651495\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 595: 0.651443\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 596: 0.651391\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 597: 0.651335\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 598: 0.651261\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 599: 0.651187\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 600: 0.651115\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 601: 0.651043\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 602: 0.650974\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 603: 0.650905\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 604: 0.650837\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 605: 0.650771\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 606: 0.650705\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 607: 0.650641\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 608: 0.650577\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 609: 0.650514\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 610: 0.650452\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 611: 0.650391\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 612: 0.650330\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 613: 0.650271\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 614: 0.650212\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 615: 0.650153\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 616: 0.650096\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 617: 0.650038\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 618: 0.649981\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 619: 0.649925\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 620: 0.649869\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 621: 0.649814\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 622: 0.649759\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 623: 0.649705\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 624: 0.649651\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 625: 0.649598\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 626: 0.649544\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 627: 0.649492\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 628: 0.649439\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 629: 0.649387\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 630: 0.649336\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 631: 0.649284\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 632: 0.649233\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 633: 0.649182\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 634: 0.649132\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 635: 0.649082\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 636: 0.649032\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 637: 0.648975\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 638: 0.648900\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 639: 0.648825\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 640: 0.648752\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 641: 0.648681\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 642: 0.648611\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 643: 0.648543\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 644: 0.648476\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 645: 0.648410\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 646: 0.648346\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 647: 0.648283\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 648: 0.648220\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 649: 0.648159\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 650: 0.648099\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 651: 0.648039\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 652: 0.647981\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 653: 0.647923\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 654: 0.647866\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 655: 0.647809\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 656: 0.647754\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 657: 0.647698\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 658: 0.647644\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 659: 0.647590\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 660: 0.647537\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 661: 0.647484\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 662: 0.647431\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 663: 0.647380\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 664: 0.647328\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 665: 0.647277\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 666: 0.647226\n",
      "Minibatch accuracy: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 667: 0.647176\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 668: 0.647126\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 669: 0.647076\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 670: 0.647027\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 671: 0.646978\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 672: 0.646930\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 673: 0.646882\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 674: 0.646834\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 675: 0.646786\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 676: 0.646738\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 677: 0.646691\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 678: 0.646645\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 679: 0.646597\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 680: 0.646551\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 681: 0.646495\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 682: 0.646411\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 683: 0.646328\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 684: 0.646248\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 685: 0.646172\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 686: 0.646097\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 687: 0.646026\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 688: 0.645957\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 689: 0.645890\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 690: 0.645825\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 691: 0.645759\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 692: 0.645658\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 693: 0.645559\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 694: 0.645466\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 695: 0.645378\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 696: 0.645293\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 697: 0.645213\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 698: 0.645135\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 699: 0.645061\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 700: 0.644990\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 701: 0.644920\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 702: 0.644854\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 703: 0.644789\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 704: 0.644726\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 705: 0.644664\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 706: 0.644604\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 707: 0.644545\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 708: 0.644488\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 709: 0.644432\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 710: 0.644376\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 711: 0.644322\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 712: 0.644269\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 713: 0.644216\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 714: 0.644164\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 715: 0.644113\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 716: 0.644063\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 717: 0.644013\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 718: 0.643964\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 719: 0.643915\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 720: 0.643867\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 721: 0.643819\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 722: 0.643772\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 723: 0.643726\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 724: 0.643679\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 725: 0.643633\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 726: 0.643588\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 727: 0.643543\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 728: 0.643497\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 729: 0.643453\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 730: 0.643409\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 731: 0.643365\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 732: 0.643321\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 733: 0.643277\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 734: 0.643234\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 735: 0.643191\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 736: 0.643148\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 737: 0.643106\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 738: 0.643063\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 739: 0.643021\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 740: 0.642979\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 741: 0.642937\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 742: 0.642896\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 743: 0.642854\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 744: 0.642814\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 745: 0.642772\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 746: 0.642731\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 747: 0.642690\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 748: 0.642650\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 749: 0.642609\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 750: 0.642569\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 751: 0.642528\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 752: 0.642489\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 753: 0.642448\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 754: 0.642409\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 755: 0.642369\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 756: 0.642329\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 757: 0.642290\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 758: 0.642250\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 759: 0.642211\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 760: 0.642172\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 761: 0.642133\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 762: 0.642094\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 763: 0.642055\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 764: 0.642016\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 765: 0.641977\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 766: 0.641939\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 767: 0.641900\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 768: 0.641862\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 769: 0.641823\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 770: 0.641785\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 771: 0.641747\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 772: 0.641709\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 773: 0.641671\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 774: 0.641633\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 775: 0.641595\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 776: 0.641557\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 777: 0.641520\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 778: 0.641482\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 779: 0.641444\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 780: 0.641407\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 781: 0.641369\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 782: 0.641332\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 783: 0.641295\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 784: 0.641258\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 785: 0.641221\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 786: 0.641183\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 787: 0.641147\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 788: 0.641110\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 789: 0.641073\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 790: 0.641036\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 791: 0.640999\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 792: 0.640963\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 793: 0.640926\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 794: 0.640889\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 795: 0.640853\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 796: 0.640816\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 797: 0.640780\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 798: 0.640744\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 799: 0.640707\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 800: 0.640671\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 801: 0.640635\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 802: 0.640599\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 803: 0.640563\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 804: 0.640527\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 805: 0.640491\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 806: 0.640455\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 807: 0.640419\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 808: 0.640384\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 809: 0.640348\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 810: 0.640312\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 811: 0.640277\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 812: 0.640241\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 813: 0.640206\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 814: 0.640170\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 815: 0.640134\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 816: 0.640100\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 817: 0.640064\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 818: 0.640029\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 819: 0.639994\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 820: 0.639958\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 821: 0.639924\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 822: 0.639888\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 823: 0.639853\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 824: 0.639818\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 825: 0.639783\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 826: 0.639749\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 827: 0.639714\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 828: 0.639679\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 829: 0.639645\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 830: 0.639610\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 831: 0.639575\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 832: 0.639541\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 833: 0.639506\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 834: 0.639472\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 835: 0.639437\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 836: 0.639403\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 837: 0.639368\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 838: 0.639334\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 839: 0.639300\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 840: 0.639266\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 841: 0.639231\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 842: 0.639197\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 843: 0.639163\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 844: 0.639129\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 845: 0.639095\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 846: 0.639061\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 847: 0.639027\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 848: 0.638993\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 849: 0.638959\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 850: 0.638926\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 851: 0.638892\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 852: 0.638858\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 853: 0.638825\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 854: 0.638791\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 855: 0.638758\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 856: 0.638724\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 857: 0.638690\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 858: 0.638657\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 859: 0.638624\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 860: 0.638590\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 861: 0.638557\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 862: 0.638524\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 863: 0.638490\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 864: 0.638457\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 865: 0.638424\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 866: 0.638391\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 867: 0.638358\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 868: 0.638325\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 869: 0.638292\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 870: 0.638259\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 871: 0.638226\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 872: 0.638193\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 873: 0.638160\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 874: 0.638128\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 875: 0.638095\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 876: 0.638062\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 877: 0.638030\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 878: 0.637997\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 879: 0.637964\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 880: 0.637932\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 881: 0.637899\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 882: 0.637867\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 883: 0.637834\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 884: 0.637802\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 885: 0.637770\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 886: 0.637737\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 887: 0.637705\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 888: 0.637672\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 889: 0.637640\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 890: 0.637608\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 891: 0.637576\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 892: 0.637544\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 893: 0.637512\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 894: 0.637480\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 895: 0.637447\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 896: 0.637416\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 897: 0.637384\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 898: 0.637352\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 899: 0.637320\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 900: 0.637288\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 901: 0.637257\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 902: 0.637225\n",
      "Minibatch accuracy: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 903: 0.637193\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 904: 0.637162\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 905: 0.637130\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 906: 0.637098\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 907: 0.637067\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 908: 0.637035\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 909: 0.637004\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 910: 0.636972\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 911: 0.636941\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 912: 0.636909\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 913: 0.636878\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 914: 0.636847\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 915: 0.636816\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 916: 0.636784\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 917: 0.636753\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 918: 0.636722\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 919: 0.636691\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 920: 0.636660\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 921: 0.636629\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 922: 0.636598\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 923: 0.636567\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 924: 0.636536\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 925: 0.636505\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 926: 0.636474\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 927: 0.636443\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 928: 0.636412\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 929: 0.636382\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 930: 0.636351\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 931: 0.636320\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 932: 0.636290\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 933: 0.636259\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 934: 0.636228\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 935: 0.636198\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 936: 0.636167\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 937: 0.636137\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 938: 0.636106\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 939: 0.636076\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 940: 0.636046\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 941: 0.636015\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 942: 0.635985\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 943: 0.635955\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 944: 0.635924\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 945: 0.635894\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 946: 0.635864\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 947: 0.635834\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 948: 0.635804\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 949: 0.635774\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 950: 0.635744\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 951: 0.635714\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 952: 0.635684\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 953: 0.635654\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 954: 0.635624\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 955: 0.635594\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 956: 0.635564\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 957: 0.635534\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 958: 0.635505\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 959: 0.635475\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 960: 0.635445\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 961: 0.635415\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 962: 0.635386\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 963: 0.635356\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 964: 0.635327\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 965: 0.635297\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 966: 0.635268\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 967: 0.635238\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 968: 0.635209\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 969: 0.635179\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 970: 0.635150\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 971: 0.635121\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 972: 0.635091\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 973: 0.635062\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 974: 0.635033\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 975: 0.635003\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 976: 0.634974\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 977: 0.634945\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 978: 0.634916\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 979: 0.634887\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 980: 0.634858\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 981: 0.634829\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 982: 0.634800\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 983: 0.634771\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 984: 0.634742\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 985: 0.634713\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 986: 0.634684\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 987: 0.634655\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 988: 0.634626\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 989: 0.634598\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 990: 0.634569\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 991: 0.634540\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 992: 0.634512\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 993: 0.634483\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 994: 0.634454\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 995: 0.634426\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 996: 0.634397\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 997: 0.634369\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 998: 0.634340\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 999: 0.634312\n",
      "Minibatch accuracy: 0.0%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1169</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5951</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2096</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7882</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4870</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9055</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2835</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6948</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3059</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5234</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1295</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4308</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1567</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1199</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1403</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1282</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2424</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8072</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12579</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3430</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2134</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2647</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2241</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1804</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2069</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1374</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>426</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>409</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2415</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6836</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1514</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7393</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1193</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7297</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2831</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1258</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>753</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2427</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2538</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8386</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4844</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2923</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8229</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2028</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1433</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6289</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1409</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6579</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1743</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3565</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1569</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1936</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3959</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2390</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1736</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3857</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>804</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1845</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4576</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1   2   3   4   5   6   7   8      9  ...  20  21  22  23  24  25  \\\n",
       "0     0   6   4   0   0   0   1   0   0   1169 ...   0  67   0   0   1   2   \n",
       "1     1  48   2   0   0   0   1   0   0   5951 ...   0  22   0   0   1   1   \n",
       "2     3  12   4   0   0   0   0   1   0   2096 ...   0  49   0   0   1   1   \n",
       "3     0  42   2   0   0   1   0   0   0   7882 ...   0  45   0   0   0   1   \n",
       "4     0  24   3   1   0   0   0   0   0   4870 ...   1  53   0   0   0   2   \n",
       "5     3  36   2   0   0   0   0   1   0   9055 ...   1  35   0   0   0   1   \n",
       "6     3  24   2   0   0   1   0   0   0   2835 ...   0  53   0   0   1   1   \n",
       "7     1  36   2   0   1   0   0   0   0   6948 ...   0  35   0   1   0   1   \n",
       "8     3  12   2   0   0   0   1   0   0   3059 ...   0  61   0   0   1   1   \n",
       "9     1  30   4   1   0   0   0   0   0   5234 ...   0  28   0   0   1   2   \n",
       "10    1  12   2   1   0   0   0   0   0   1295 ...   0  25   0   1   0   1   \n",
       "11    0  48   2   0   0   0   0   0   1   4308 ...   0  24   0   1   0   1   \n",
       "12    1  12   2   0   0   0   1   0   0   1567 ...   0  22   0   0   1   1   \n",
       "13    0  24   4   1   0   0   0   0   0   1199 ...   0  60   0   0   1   2   \n",
       "14    0  15   2   1   0   0   0   0   0   1403 ...   0  28   0   1   0   1   \n",
       "15    0  24   2   0   0   0   1   0   0   1282 ...   0  32   0   0   1   1   \n",
       "16    3  24   4   0   0   0   1   0   0   2424 ...   0  53   0   0   1   2   \n",
       "17    0  30   0   0   0   0   0   0   1   8072 ...   0  25   1   0   1   3   \n",
       "18    1  24   2   0   1   0   0   0   0  12579 ...   1  44   0   0   0   1   \n",
       "19    3  24   2   0   0   0   1   0   0   3430 ...   0  31   0   0   1   1   \n",
       "20    3   9   4   1   0   0   0   0   0   2134 ...   0  48   0   0   1   3   \n",
       "21    0   6   2   0   0   0   1   0   0   2647 ...   0  44   0   1   0   1   \n",
       "22    0  10   4   1   0   0   0   0   0   2241 ...   0  48   0   1   0   2   \n",
       "23    1  12   4   0   1   0   0   0   0   1804 ...   0  44   0   0   1   1   \n",
       "24    3  10   4   0   0   1   0   0   0   2069 ...   0  26   0   0   1   2   \n",
       "25    0   6   2   0   0   1   0   0   0   1374 ...   0  36   1   0   1   1   \n",
       "26    3   6   0   0   0   0   1   0   0    426 ...   0  39   0   0   1   1   \n",
       "27    2  12   1   0   0   0   1   0   0    409 ...   0  42   0   1   0   2   \n",
       "28    1   7   2   0   0   0   1   0   0   2415 ...   0  34   0   0   1   1   \n",
       "29    0  60   3   0   0   0   0   0   1   6836 ...   1  63   0   0   1   2   \n",
       "..   ..  ..  ..  ..  ..  ..  ..  ..  ..    ... ...  ..  ..  ..  ..  ..  ..   \n",
       "970   1  15   2   0   0   0   0   0   0   1514 ...   0  22   0   0   1   1   \n",
       "971   3  24   2   1   0   0   0   0   0   7393 ...   0  43   0   0   1   1   \n",
       "972   0  24   1   1   0   0   0   0   0   1193 ...   1  29   0   1   0   2   \n",
       "973   0  60   2   0   0   0   0   0   1   7297 ...   1  36   0   1   0   1   \n",
       "974   3  30   4   0   0   0   1   0   0   2831 ...   0  33   0   0   1   1   \n",
       "975   2  24   2   0   0   0   1   0   0   1258 ...   0  57   0   0   1   1   \n",
       "976   1   6   2   0   0   0   1   0   0    753 ...   0  64   0   0   1   1   \n",
       "977   1  18   3   0   0   0   0   0   1   2427 ...   0  42   0   0   1   2   \n",
       "978   3  24   3   1   0   0   0   0   0   2538 ...   0  47   0   0   1   2   \n",
       "979   1  15   1   1   0   0   0   0   0   1264 ...   0  25   0   1   0   1   \n",
       "980   1  30   4   0   0   1   0   0   0   8386 ...   0  49   0   0   1   1   \n",
       "981   3  48   2   0   0   0   0   0   1   4844 ...   0  33   1   1   0   1   \n",
       "982   2  21   2   1   0   0   0   0   0   2923 ...   0  28   1   0   1   1   \n",
       "983   0  36   2   0   1   0   0   0   0   8229 ...   0  26   0   0   1   1   \n",
       "984   3  24   4   0   0   1   0   0   0   2028 ...   0  30   0   0   1   2   \n",
       "985   0  15   4   0   0   1   0   0   0   1433 ...   0  25   0   1   0   2   \n",
       "986   2  42   0   0   0   0   0   0   1   6289 ...   0  33   0   0   1   2   \n",
       "987   3  13   2   0   0   0   1   0   0   1409 ...   0  64   0   0   1   1   \n",
       "988   0  24   2   0   1   0   0   0   0   6579 ...   1  29   0   0   0   1   \n",
       "989   1  24   4   0   0   0   1   0   0   1743 ...   0  48   0   0   1   2   \n",
       "990   3  12   4   0   0   0   0   1   0   3565 ...   0  37   0   0   1   2   \n",
       "991   3  15   1   0   0   0   1   0   0   1569 ...   0  34   1   0   1   1   \n",
       "992   0  18   2   0   0   0   1   0   0   1936 ...   0  23   0   1   0   2   \n",
       "993   0  36   2   0   0   1   0   0   0   3959 ...   0  30   0   0   1   1   \n",
       "994   3  12   2   1   0   0   0   0   0   2390 ...   0  50   0   0   1   1   \n",
       "995   3  12   2   0   0   1   0   0   0   1736 ...   0  31   0   0   1   1   \n",
       "996   0  30   2   0   1   0   0   0   0   3857 ...   0  40   0   0   1   1   \n",
       "997   3  12   2   0   0   0   1   0   0    804 ...   0  38   0   0   1   1   \n",
       "998   0  45   2   0   0   0   1   0   0   1845 ...   1  23   0   0   0   1   \n",
       "999   1  45   4   0   1   0   0   0   0   4576 ...   0  27   0   0   1   1   \n",
       "\n",
       "     26  27  28  29  \n",
       "0     2   1   1   0  \n",
       "1     2   1   0   0  \n",
       "2     1   2   0   0  \n",
       "3     2   2   0   0  \n",
       "4     2   2   0   0  \n",
       "5     1   2   1   0  \n",
       "6     2   1   0   0  \n",
       "7     3   1   1   0  \n",
       "8     1   1   0   0  \n",
       "9     3   1   0   0  \n",
       "10    2   1   0   0  \n",
       "11    2   1   0   0  \n",
       "12    2   1   1   0  \n",
       "13    1   1   0   0  \n",
       "14    2   1   0   0  \n",
       "15    1   1   0   0  \n",
       "16    2   1   0   0  \n",
       "17    2   1   0   0  \n",
       "18    3   1   1   0  \n",
       "19    2   2   1   0  \n",
       "20    2   1   1   0  \n",
       "21    2   2   0   0  \n",
       "22    1   2   0   1  \n",
       "23    2   1   0   0  \n",
       "24    2   1   0   1  \n",
       "25    1   1   1   0  \n",
       "26    1   1   0   0  \n",
       "27    2   1   0   0  \n",
       "28    2   1   0   0  \n",
       "29    2   1   1   0  \n",
       "..   ..  ..  ..  ..  \n",
       "970   2   1   0   0  \n",
       "971   1   2   0   0  \n",
       "972   0   1   0   0  \n",
       "973   2   1   0   0  \n",
       "974   2   1   1   0  \n",
       "975   1   1   0   0  \n",
       "976   2   1   0   0  \n",
       "977   2   1   0   0  \n",
       "978   1   2   0   0  \n",
       "979   2   1   0   0  \n",
       "980   2   1   0   0  \n",
       "981   3   1   1   0  \n",
       "982   3   1   1   0  \n",
       "983   2   2   0   0  \n",
       "984   1   1   0   0  \n",
       "985   2   1   0   0  \n",
       "986   2   1   0   0  \n",
       "987   2   1   0   0  \n",
       "988   3   1   1   0  \n",
       "989   1   1   0   0  \n",
       "990   1   2   0   0  \n",
       "991   1   2   0   0  \n",
       "992   1   1   0   0  \n",
       "993   3   1   1   0  \n",
       "994   2   1   1   0  \n",
       "995   1   1   0   0  \n",
       "996   3   1   1   0  \n",
       "997   2   1   0   0  \n",
       "998   2   1   1   0  \n",
       "999   2   1   0   0  \n",
       "\n",
       "[1000 rows x 30 columns]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = pd.read_excel('GermanCreditInput.xls',header=None)\n",
    "# x_train[[0,1,2,3,4,6,9,10,11,12,13,14,17,19,20,21,22,24,27,29]]\n",
    "y_train = pd.read_excel('GermanCreditOutputClass1columnknn.xls',header=None)\n",
    "y_train = (np.arange(2) == np.array(y_train)[:]).astype(np.float32)\n",
    "tens_train(np.array(x_train),np.array(y_train),90,1000,.001,.1)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[2],[4],[6]])[2,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_train(X,Y,hiddenNum=40,trainCycles=450,lr=.1,m=.5) :\n",
    "    \"\"\"Train one layer feedforward neural network\n",
    "    Args :\n",
    "       X : training data\n",
    "       Y : training label\n",
    "       hiddenNum : number of hidden units of hidden layer\n",
    "       trainCycles : number of training cycles\n",
    "       lr : learning rate of nueral network\n",
    "       m: momentum of neural network\n",
    "    Returns :\n",
    "       'float' accuracy\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hiddenNum, input_dim=X.shape[1], kernel_initializer='normal', activation='selu'))\n",
    "#     model.add(Dense(hiddenNum, kernel_initializer='normal', activation='selu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    sgd = SGD(lr=lr, momentum=m)\n",
    "    ADAM = adam(lr=lr)\n",
    "    # loss could be \"mse\" too\n",
    "    model.compile(loss='binary_crossentropy',metrics=['binary_accuracy'],optimizer=sgd)\n",
    "    for i in range(trainCycles) :\n",
    "        combined = list(zip(X, Y))\n",
    "        random.shuffle(combined)\n",
    "        X[:], Y[:] = zip(*combined)\n",
    "        model.fit(X,Y,batch_size=len(X))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 9.3716 - binary_accuracy: 0.2467\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 4.1370 - binary_accuracy: 0.7433\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 4.0295 - binary_accuracy: 0.7500\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 4.1370 - binary_accuracy: 0.7433\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 4.1638 - binary_accuracy: 0.7417\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 4.0295 - binary_accuracy: 0.7500\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 4.1370 - binary_accuracy: 0.7433\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.8952 - binary_accuracy: 0.7583\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.1968 - binary_accuracy: 0.8017\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 12us/step - loss: 2.6595 - binary_accuracy: 0.8350\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 6us/step - loss: 3.0356 - binary_accuracy: 0.8117\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 13us/step - loss: 3.2505 - binary_accuracy: 0.7983\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.4654 - binary_accuracy: 0.7850\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 3.7609 - binary_accuracy: 0.7667\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 4.1638 - binary_accuracy: 0.7417\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 4.2982 - binary_accuracy: 0.7333\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 3.6534 - binary_accuracy: 0.7733\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.1162 - binary_accuracy: 0.8067\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 3.2505 - binary_accuracy: 0.7983\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.1968 - binary_accuracy: 0.8017\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 2.9281 - binary_accuracy: 0.8183\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 6us/step - loss: 3.1162 - binary_accuracy: 0.8067\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 3.3848 - binary_accuracy: 0.7900\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 3.3311 - binary_accuracy: 0.7933\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 2.8475 - binary_accuracy: 0.8233\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.2773 - binary_accuracy: 0.7967\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.8475 - binary_accuracy: 0.8233\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 2.3371 - binary_accuracy: 0.8550\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.8536 - binary_accuracy: 0.8850\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.4238 - binary_accuracy: 0.9117\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.4238 - binary_accuracy: 0.9117\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.1551 - binary_accuracy: 0.9283\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 0.9939 - binary_accuracy: 0.9383\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 0.9134 - binary_accuracy: 0.9433\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 0.8596 - binary_accuracy: 0.9467\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 12us/step - loss: 0.7253 - binary_accuracy: 0.9550\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 1.0745 - binary_accuracy: 0.9333\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.0208 - binary_accuracy: 0.9367\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 0.8865 - binary_accuracy: 0.9450\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 0.8596 - binary_accuracy: 0.9467\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 12us/step - loss: 0.8328 - binary_accuracy: 0.9483\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 13us/step - loss: 0.5910 - binary_accuracy: 0.9633\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 0.8059 - binary_accuracy: 0.9500\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 0.7253 - binary_accuracy: 0.9550\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 0.7522 - binary_accuracy: 0.9533\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 0.7522 - binary_accuracy: 0.9533\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 0.8865 - binary_accuracy: 0.9450\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 6us/step - loss: 0.8059 - binary_accuracy: 0.9500\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 0.8865 - binary_accuracy: 0.9450\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.1283 - binary_accuracy: 0.9300\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 1.0208 - binary_accuracy: 0.9367\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.0745 - binary_accuracy: 0.9333\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.2089 - binary_accuracy: 0.9250\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 1.2894 - binary_accuracy: 0.9200\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 1.2089 - binary_accuracy: 0.9250\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.1014 - binary_accuracy: 0.9317\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 0.7253 - binary_accuracy: 0.9550\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 0.7522 - binary_accuracy: 0.9533\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 1.2894 - binary_accuracy: 0.9200\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.2626 - binary_accuracy: 0.9217\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.1014 - binary_accuracy: 0.9317\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 1.5581 - binary_accuracy: 0.9033\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.9610 - binary_accuracy: 0.8783\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 2.1759 - binary_accuracy: 0.8650\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 2.0416 - binary_accuracy: 0.8733\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.7193 - binary_accuracy: 0.8933\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.7461 - binary_accuracy: 0.8917\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.8804 - binary_accuracy: 0.8833\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 1.8267 - binary_accuracy: 0.8867\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.5849 - binary_accuracy: 0.9017\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 1.4775 - binary_accuracy: 0.9083\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.3163 - binary_accuracy: 0.9183\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 0.9402 - binary_accuracy: 0.9417\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 1.0477 - binary_accuracy: 0.9350\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 6us/step - loss: 1.1283 - binary_accuracy: 0.9300\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.0208 - binary_accuracy: 0.9367\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.1014 - binary_accuracy: 0.9317\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.0477 - binary_accuracy: 0.9350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.1283 - binary_accuracy: 0.9300\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.0208 - binary_accuracy: 0.9367\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 1.2357 - binary_accuracy: 0.9233\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 1.3432 - binary_accuracy: 0.9167\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.4506 - binary_accuracy: 0.9100\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 6us/step - loss: 1.2894 - binary_accuracy: 0.9200\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 1.3700 - binary_accuracy: 0.9150\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.5312 - binary_accuracy: 0.9050\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.4506 - binary_accuracy: 0.9100\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.6655 - binary_accuracy: 0.8967\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 13us/step - loss: 1.6387 - binary_accuracy: 0.8983\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.7193 - binary_accuracy: 0.8933\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.4775 - binary_accuracy: 0.9083\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.2894 - binary_accuracy: 0.9200\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.3432 - binary_accuracy: 0.9167\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.4238 - binary_accuracy: 0.9117\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.2357 - binary_accuracy: 0.9233\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.4238 - binary_accuracy: 0.9117\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.1283 - binary_accuracy: 0.9300\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 1.0208 - binary_accuracy: 0.9367\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 0.9671 - binary_accuracy: 0.9400\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.0745 - binary_accuracy: 0.9333\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.3969 - binary_accuracy: 0.9133\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 1.3163 - binary_accuracy: 0.9183\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.3432 - binary_accuracy: 0.9167\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 1.7730 - binary_accuracy: 0.8900\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 1.5312 - binary_accuracy: 0.9050\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.4238 - binary_accuracy: 0.9117\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 1.4238 - binary_accuracy: 0.9117\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.6655 - binary_accuracy: 0.8967\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.6655 - binary_accuracy: 0.8967\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 1.4506 - binary_accuracy: 0.9100\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.3432 - binary_accuracy: 0.9167\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 6us/step - loss: 1.4506 - binary_accuracy: 0.9100\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.5849 - binary_accuracy: 0.9017\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 1.7999 - binary_accuracy: 0.8883\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.0148 - binary_accuracy: 0.8750\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 1.8536 - binary_accuracy: 0.8850\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 1.5312 - binary_accuracy: 0.9050\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.3700 - binary_accuracy: 0.9150\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 1.5312 - binary_accuracy: 0.9050\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 6us/step - loss: 1.6118 - binary_accuracy: 0.9000\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.7193 - binary_accuracy: 0.8933\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 2.1491 - binary_accuracy: 0.8667\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 2.0954 - binary_accuracy: 0.8700\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 2.0148 - binary_accuracy: 0.8750\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.8267 - binary_accuracy: 0.8867\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.9073 - binary_accuracy: 0.8817\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.9342 - binary_accuracy: 0.8800\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.8536 - binary_accuracy: 0.8850\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 2.2565 - binary_accuracy: 0.8600\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 2.5520 - binary_accuracy: 0.8417\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.0416 - binary_accuracy: 0.8733\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.8536 - binary_accuracy: 0.8850\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.5849 - binary_accuracy: 0.9017\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.6655 - binary_accuracy: 0.8967\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.5849 - binary_accuracy: 0.9017\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 1.4775 - binary_accuracy: 0.9083\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 1.8804 - binary_accuracy: 0.8833\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.7461 - binary_accuracy: 0.8917\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 2.0416 - binary_accuracy: 0.8733\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 1.9342 - binary_accuracy: 0.8800\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.9342 - binary_accuracy: 0.8800\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 2.3371 - binary_accuracy: 0.8550\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 2.2834 - binary_accuracy: 0.8583\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 1.9073 - binary_accuracy: 0.8817\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.3969 - binary_accuracy: 0.9133\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.4506 - binary_accuracy: 0.9100\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.8536 - binary_accuracy: 0.8850\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.6118 - binary_accuracy: 0.9000\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 1.5044 - binary_accuracy: 0.9067\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 6us/step - loss: 1.4238 - binary_accuracy: 0.9117\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.7461 - binary_accuracy: 0.8917\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.7730 - binary_accuracy: 0.8900\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 2.0416 - binary_accuracy: 0.8733\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.7999 - binary_accuracy: 0.8883\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.5252 - binary_accuracy: 0.8433\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.2028 - binary_accuracy: 0.8633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 13us/step - loss: 2.0954 - binary_accuracy: 0.8700\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 2.2028 - binary_accuracy: 0.8633\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.0685 - binary_accuracy: 0.8717\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.3909 - binary_accuracy: 0.8517\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.7461 - binary_accuracy: 0.8917\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 1.7193 - binary_accuracy: 0.8933\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.6924 - binary_accuracy: 0.8950\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.6118 - binary_accuracy: 0.9000\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.9342 - binary_accuracy: 0.8800\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 1.6655 - binary_accuracy: 0.8967\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 6us/step - loss: 1.5312 - binary_accuracy: 0.9050\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.7730 - binary_accuracy: 0.8900\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.8267 - binary_accuracy: 0.8867\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.3103 - binary_accuracy: 0.8567\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 2.1222 - binary_accuracy: 0.8683\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 2.2565 - binary_accuracy: 0.8600\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.6326 - binary_accuracy: 0.8367\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 2.3371 - binary_accuracy: 0.8550\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 1.8267 - binary_accuracy: 0.8867\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.8267 - binary_accuracy: 0.8867\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.9073 - binary_accuracy: 0.8817\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.9610 - binary_accuracy: 0.8783\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.8536 - binary_accuracy: 0.8850\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 2.2028 - binary_accuracy: 0.8633\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.3909 - binary_accuracy: 0.8517\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.6595 - binary_accuracy: 0.8350\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.3103 - binary_accuracy: 0.8567\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.9342 - binary_accuracy: 0.8800\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 2.0148 - binary_accuracy: 0.8750\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.9879 - binary_accuracy: 0.8767\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 2.0416 - binary_accuracy: 0.8733\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 1.9879 - binary_accuracy: 0.8767\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 14us/step - loss: 2.0148 - binary_accuracy: 0.8750\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 13us/step - loss: 2.1222 - binary_accuracy: 0.8683\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 2.0416 - binary_accuracy: 0.8733\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 13us/step - loss: 2.3371 - binary_accuracy: 0.8550\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 12us/step - loss: 1.7461 - binary_accuracy: 0.8917\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.2028 - binary_accuracy: 0.8633\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.1222 - binary_accuracy: 0.8683\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 2.0954 - binary_accuracy: 0.8700\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 2.6863 - binary_accuracy: 0.8333\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 2.9013 - binary_accuracy: 0.8200\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.1430 - binary_accuracy: 0.8050\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 3.0356 - binary_accuracy: 0.8117\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.7401 - binary_accuracy: 0.8300\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 2.8207 - binary_accuracy: 0.8250\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 3.1162 - binary_accuracy: 0.8067\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 2.9013 - binary_accuracy: 0.8200\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.7132 - binary_accuracy: 0.8317\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 2.5520 - binary_accuracy: 0.8417\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.7938 - binary_accuracy: 0.8267\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 2.5789 - binary_accuracy: 0.8400\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.7669 - binary_accuracy: 0.8283\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.2834 - binary_accuracy: 0.8583\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 2.2565 - binary_accuracy: 0.8600\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 2.6058 - binary_accuracy: 0.8383\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.2565 - binary_accuracy: 0.8600\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 6us/step - loss: 2.5789 - binary_accuracy: 0.8400\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 2.6326 - binary_accuracy: 0.8367\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 6us/step - loss: 2.9013 - binary_accuracy: 0.8200\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 3.1430 - binary_accuracy: 0.8050\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 6us/step - loss: 3.2773 - binary_accuracy: 0.7967\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.7340 - binary_accuracy: 0.7683\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 3.6266 - binary_accuracy: 0.7750\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 3.9489 - binary_accuracy: 0.7550\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 6us/step - loss: 4.4593 - binary_accuracy: 0.7233\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 6us/step - loss: 4.7011 - binary_accuracy: 0.7083\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 4.7817 - binary_accuracy: 0.7033\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 6us/step - loss: 4.4056 - binary_accuracy: 0.7267\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 4.2713 - binary_accuracy: 0.7350\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 3.3848 - binary_accuracy: 0.7900\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.3042 - binary_accuracy: 0.7950\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 3.3311 - binary_accuracy: 0.7933\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.1162 - binary_accuracy: 0.8067\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.4654 - binary_accuracy: 0.7850\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.5728 - binary_accuracy: 0.7783\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.6266 - binary_accuracy: 0.7750\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 3.5997 - binary_accuracy: 0.7767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.9489 - binary_accuracy: 0.7550\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 6us/step - loss: 3.6534 - binary_accuracy: 0.7733\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.8146 - binary_accuracy: 0.7633\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.9221 - binary_accuracy: 0.7567\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 4.2176 - binary_accuracy: 0.7383\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 4.3787 - binary_accuracy: 0.7283\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 6us/step - loss: 4.6742 - binary_accuracy: 0.7100\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 4.4862 - binary_accuracy: 0.7217\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 4.2176 - binary_accuracy: 0.7383\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 4.1370 - binary_accuracy: 0.7433\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 3.9489 - binary_accuracy: 0.7550\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 5us/step - loss: 3.7878 - binary_accuracy: 0.7650\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 3.7072 - binary_accuracy: 0.7700\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 3.8952 - binary_accuracy: 0.7583\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 6us/step - loss: 4.5399 - binary_accuracy: 0.7183\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 6us/step - loss: 4.4325 - binary_accuracy: 0.7250\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 6us/step - loss: 4.5131 - binary_accuracy: 0.7200\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 4.6474 - binary_accuracy: 0.7117\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 5.1847 - binary_accuracy: 0.6783\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 5.1847 - binary_accuracy: 0.6783\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 5.5607 - binary_accuracy: 0.6550\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 6.2055 - binary_accuracy: 0.6150\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 6.1517 - binary_accuracy: 0.6183\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 12us/step - loss: 7.0920 - binary_accuracy: 0.5600\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 7.3337 - binary_accuracy: 0.5450\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 7.1188 - binary_accuracy: 0.5583\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 12us/step - loss: 7.1188 - binary_accuracy: 0.5583\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 6.8233 - binary_accuracy: 0.5767\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 6.6890 - binary_accuracy: 0.5850\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 5.9906 - binary_accuracy: 0.6283\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 5.8025 - binary_accuracy: 0.6400\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 5.8562 - binary_accuracy: 0.6367\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 5.5070 - binary_accuracy: 0.6583\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 5.6145 - binary_accuracy: 0.6517\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 5.7219 - binary_accuracy: 0.6450\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 5.8025 - binary_accuracy: 0.6400\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 6.4204 - binary_accuracy: 0.6017\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 6.6353 - binary_accuracy: 0.5883\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 6.8771 - binary_accuracy: 0.5733\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 12us/step - loss: 7.2800 - binary_accuracy: 0.5483\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 6.9576 - binary_accuracy: 0.5683\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 6.5278 - binary_accuracy: 0.5950\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 5.9100 - binary_accuracy: 0.6333\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 5.9368 - binary_accuracy: 0.6317\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 5.8562 - binary_accuracy: 0.6367\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 6.3935 - binary_accuracy: 0.6033\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 5.8294 - binary_accuracy: 0.6383\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 5.9906 - binary_accuracy: 0.6283\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 5.9100 - binary_accuracy: 0.6333\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 5.2115 - binary_accuracy: 0.6767\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 4.7548 - binary_accuracy: 0.7050\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 5.0503 - binary_accuracy: 0.6867\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 4.0833 - binary_accuracy: 0.7467\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 3.7340 - binary_accuracy: 0.7683\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 4.0833 - binary_accuracy: 0.7467\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 4.0833 - binary_accuracy: 0.7467\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.8683 - binary_accuracy: 0.7600\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 3.3579 - binary_accuracy: 0.7917\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.2773 - binary_accuracy: 0.7967\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 3.0624 - binary_accuracy: 0.8100\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.9013 - binary_accuracy: 0.8200\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.8744 - binary_accuracy: 0.8217\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 2.8744 - binary_accuracy: 0.8217\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.4714 - binary_accuracy: 0.8467\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.7669 - binary_accuracy: 0.8283\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 2.6595 - binary_accuracy: 0.8350\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.7669 - binary_accuracy: 0.8283\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 2.9281 - binary_accuracy: 0.8183\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 2.7401 - binary_accuracy: 0.8300\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 2.9818 - binary_accuracy: 0.8150\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 3.1430 - binary_accuracy: 0.8050\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.3579 - binary_accuracy: 0.7917\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.0624 - binary_accuracy: 0.8100\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 3.2505 - binary_accuracy: 0.7983\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 3.1699 - binary_accuracy: 0.8033\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 3.3042 - binary_accuracy: 0.7950\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 3.0893 - binary_accuracy: 0.8083\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.5520 - binary_accuracy: 0.8417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 2.7132 - binary_accuracy: 0.8317\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.3640 - binary_accuracy: 0.8533\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 2.2028 - binary_accuracy: 0.8633\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 14us/step - loss: 2.3371 - binary_accuracy: 0.8550\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 2.0416 - binary_accuracy: 0.8733\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.0685 - binary_accuracy: 0.8717\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 6us/step - loss: 2.0416 - binary_accuracy: 0.8733\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 2.0148 - binary_accuracy: 0.8750\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.0685 - binary_accuracy: 0.8717\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 2.5252 - binary_accuracy: 0.8433\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 2.8475 - binary_accuracy: 0.8233\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 3.2505 - binary_accuracy: 0.7983\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 2.9281 - binary_accuracy: 0.8183\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 2.9550 - binary_accuracy: 0.8167\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.6595 - binary_accuracy: 0.8350\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.8475 - binary_accuracy: 0.8233\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.9550 - binary_accuracy: 0.8167\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 2.8207 - binary_accuracy: 0.8250\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.2028 - binary_accuracy: 0.8633\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 2.4714 - binary_accuracy: 0.8467\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.9879 - binary_accuracy: 0.8767\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 6us/step - loss: 1.7461 - binary_accuracy: 0.8917\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.5044 - binary_accuracy: 0.9067\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.4775 - binary_accuracy: 0.9083\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.6387 - binary_accuracy: 0.8983\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.7193 - binary_accuracy: 0.8933\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.4775 - binary_accuracy: 0.9083\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 1.9342 - binary_accuracy: 0.8800\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 1.8267 - binary_accuracy: 0.8867\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.7193 - binary_accuracy: 0.8933\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 1.9879 - binary_accuracy: 0.8767\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.8267 - binary_accuracy: 0.8867\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 1.6655 - binary_accuracy: 0.8967\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.6924 - binary_accuracy: 0.8950\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.4506 - binary_accuracy: 0.9100\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 1.5849 - binary_accuracy: 0.9017\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 1.6924 - binary_accuracy: 0.8950\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.7193 - binary_accuracy: 0.8933\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 13us/step - loss: 1.5581 - binary_accuracy: 0.9033\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.4775 - binary_accuracy: 0.9083\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.4775 - binary_accuracy: 0.9083\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 1.3969 - binary_accuracy: 0.9133\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 1.3700 - binary_accuracy: 0.9150\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 1.6655 - binary_accuracy: 0.8967\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 2.0416 - binary_accuracy: 0.8733\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.2297 - binary_accuracy: 0.8617\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 2.5520 - binary_accuracy: 0.8417\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 2.7938 - binary_accuracy: 0.8267\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.8207 - binary_accuracy: 0.8250\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 2.4983 - binary_accuracy: 0.8450\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 2.8475 - binary_accuracy: 0.8233\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 3.0356 - binary_accuracy: 0.8117\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 3.0893 - binary_accuracy: 0.8083\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.0893 - binary_accuracy: 0.8083\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 3.5997 - binary_accuracy: 0.7767\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 3.4385 - binary_accuracy: 0.7867\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 3.1430 - binary_accuracy: 0.8050\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 2.9550 - binary_accuracy: 0.8167\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 2.9013 - binary_accuracy: 0.8200\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 2.9818 - binary_accuracy: 0.8150\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 2.8207 - binary_accuracy: 0.8250\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.2773 - binary_accuracy: 0.7967\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.2505 - binary_accuracy: 0.7983\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 3.4923 - binary_accuracy: 0.7833\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 12us/step - loss: 3.6266 - binary_accuracy: 0.7750\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 4.0295 - binary_accuracy: 0.7500\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 4.4056 - binary_accuracy: 0.7267\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 4.6205 - binary_accuracy: 0.7133\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 6us/step - loss: 5.2384 - binary_accuracy: 0.6750\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 5.4802 - binary_accuracy: 0.6600\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 5.0235 - binary_accuracy: 0.6883\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 5.3727 - binary_accuracy: 0.6667\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 5.4533 - binary_accuracy: 0.6617\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 5.0772 - binary_accuracy: 0.6850\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 4.5668 - binary_accuracy: 0.7167\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 4.8892 - binary_accuracy: 0.6967\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 4.9966 - binary_accuracy: 0.6900\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 5.0772 - binary_accuracy: 0.6850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 5.2921 - binary_accuracy: 0.6717\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 5.9100 - binary_accuracy: 0.6333\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 6.0711 - binary_accuracy: 0.6233\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 6.4204 - binary_accuracy: 0.6017\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 6.9576 - binary_accuracy: 0.5683\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 6.3666 - binary_accuracy: 0.6050\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 6.5010 - binary_accuracy: 0.5967\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 6.4741 - binary_accuracy: 0.5983\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 12us/step - loss: 6.4204 - binary_accuracy: 0.6017\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 6.1517 - binary_accuracy: 0.6183\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 5.5607 - binary_accuracy: 0.6550\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 5.4264 - binary_accuracy: 0.6633\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 5.0235 - binary_accuracy: 0.6883\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 5.2921 - binary_accuracy: 0.6717\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 12us/step - loss: 4.7011 - binary_accuracy: 0.7083\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 4.6742 - binary_accuracy: 0.7100\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 4.2713 - binary_accuracy: 0.7350\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 4.1101 - binary_accuracy: 0.7450\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 4.0027 - binary_accuracy: 0.7517\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 3.9758 - binary_accuracy: 0.7533\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.8683 - binary_accuracy: 0.7600\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 3.8415 - binary_accuracy: 0.7617\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 3.8415 - binary_accuracy: 0.7617\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.7609 - binary_accuracy: 0.7667\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.1430 - binary_accuracy: 0.8050\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 2.9818 - binary_accuracy: 0.8150\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 3.6266 - binary_accuracy: 0.7750\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 3.8415 - binary_accuracy: 0.7617\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 4.1907 - binary_accuracy: 0.7400\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.7878 - binary_accuracy: 0.7650\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.6266 - binary_accuracy: 0.7750\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.6266 - binary_accuracy: 0.7750\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.7878 - binary_accuracy: 0.7650\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.7609 - binary_accuracy: 0.7667\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.7609 - binary_accuracy: 0.7667\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 2.9013 - binary_accuracy: 0.8200\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 2.9281 - binary_accuracy: 0.8183\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 12us/step - loss: 2.7938 - binary_accuracy: 0.8267\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 3.1162 - binary_accuracy: 0.8067\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 3.7609 - binary_accuracy: 0.7667\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 3.3311 - binary_accuracy: 0.7933\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 3.3311 - binary_accuracy: 0.7933\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 3.2773 - binary_accuracy: 0.7967\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 3.7878 - binary_accuracy: 0.7650\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 4.0027 - binary_accuracy: 0.7517\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 4.1638 - binary_accuracy: 0.7417\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 4.5668 - binary_accuracy: 0.7167\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 4.8623 - binary_accuracy: 0.6983\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 7us/step - loss: 4.9429 - binary_accuracy: 0.6933\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 4.7011 - binary_accuracy: 0.7083\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 9us/step - loss: 4.8892 - binary_accuracy: 0.6967\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 5.4264 - binary_accuracy: 0.6633\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 5.1309 - binary_accuracy: 0.6817\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 5.2115 - binary_accuracy: 0.6767\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 5.8294 - binary_accuracy: 0.6383\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 5.9368 - binary_accuracy: 0.6317\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 11us/step - loss: 6.0443 - binary_accuracy: 0.6250\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 5.7488 - binary_accuracy: 0.6433\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 10us/step - loss: 5.5607 - binary_accuracy: 0.6550\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 8us/step - loss: 5.6145 - binary_accuracy: 0.6517\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x14c0f3f9e10>"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = pd.read_excel('GermanCreditInput.xls',header=None)\n",
    "y_train = pd.read_excel('GermanCreditOutputClass1columnknn.xls',header=None)\n",
    "# y_train = (np.arange(2) == np.array(y_train)[:]).astype(np.float32)\n",
    "X = np.append(x_train[y_train[0] == 1][:300], x_train[y_train[0] == 0][:300],axis=0)\n",
    "Y = np.append(y_train[y_train[0] == 1][:300], y_train[y_train[0] == 0][:300],axis=0)\n",
    "combined = list(zip(X, Y))\n",
    "random.shuffle(combined)\n",
    "X[:], Y[:] = zip(*combined)\n",
    "# np.sum((np.array(y_train).reshape(-1) == 1).argmax())\n",
    "# (np.array(y_train).reshape(-1) == 1)\n",
    "simple_train(np.array(X),np.array(Y),30,450,.3,.9)\n",
    "# x_train[y_train == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chromosome : \n",
    "    def __init__(self,genNum,trueProb) :\n",
    "        self.chrom = np.array([True if random() > trueProb else False for _ in range(genNum)])\n",
    "        self.fitness = None\n",
    "\n",
    "    def __iter__(self) :\n",
    "        return np.nditer(self.chrom)\n",
    "        \n",
    "    def mutate(self,prob) :\n",
    "        self.chrom = np.vectorize(lambda x:x if random() > prob else not x)(self.chrom)\n",
    "        \n",
    "    def calculateFitness(X,Y,hiddenNum=40,trainCycles=450,lr=.1,m=.5) :\n",
    "        \"\"\"Train one layer feedforward neural network\n",
    "        Args :\n",
    "           X : training data\n",
    "           Y : training label\n",
    "           hiddenNum : number of hidden units of hidden layer\n",
    "           trainCycles : number of training cycles\n",
    "           lr : learning rate of nueral network\n",
    "           m: momentum of neural network\n",
    "        Returns :\n",
    "           'float' accuracy\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Dense(hiddenNum, input_dim=X.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "        model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "        ADAM = adam(lr=lr)\n",
    "        # loss could be \"mse\" too\n",
    "        model.compile(loss='binary_crossentropy',metrics=['accuracy'],optimizer=ADAM)\n",
    "        model.fit(X,Y,epochs=trainCycles)\n",
    "        return model.accuricy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Population :\n",
    "    def __init__(self,popSize,genNum,trueProb,X,Y) :\n",
    "        self.pop = np.array([])\n",
    "        self.popSize = popSize\n",
    "        self.genNum = genNum\n",
    "        self.trueProb = trueProb\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        for _ in range(popSize) : # \n",
    "            np.append(self.pop,Chromosome(genNum,trueProb))\n",
    "        \n",
    "    def __iter__(self) :\n",
    "        return np.nditer(self.pop)\n",
    "    \n",
    "    def mutate(self,prob) :\n",
    "        for chrom in self :\n",
    "            chrom.mutate(prob)\n",
    "    \n",
    "    def crossover(self,prob) :\n",
    "        newPop = Population(self.popSize,self.genNum,self.trueProb,self.X,self.Y)\n",
    "        for i in range(len(self.pop)) :\n",
    "            rand_prob = np.random.random()\n",
    "            if rand_prob > prob :\n",
    "                first = self.pop[np.random.randint(len(self.pop))].chrom\n",
    "                second = self.pop[np.random.randint(len(self.pop))].chrom\n",
    "                point = random.randint(len(first))\n",
    "                newPop.pop[i].chrom = np.append(first[:point] + second[point:])\n",
    "            else :\n",
    "                newPop.pop[i].chrom = np.append(first[:point] + second[point:])\n",
    "        return newPop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Population(config['populationSize'],12,config['initProb'],[10,10],[2,20])\n",
    "config = { \n",
    "    'maxFeatureNum': 12,\n",
    "    'minFeatureNum': 5,\n",
    "    'populationSize': 50,\n",
    "    'initProb': .5,\n",
    "    'crossoverProb': .9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneticAlgorithm :\n",
    "    def __init__(self,initPopSize,genNum,trueProb) :\n",
    "        self.bestChrom = None\n",
    "        \n",
    "    def run(self,mProb,cProb) :\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
